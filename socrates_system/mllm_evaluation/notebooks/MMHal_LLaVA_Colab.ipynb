{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "MMHal_LLaVA_Colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MMHal-Bench with LLaVA-HF (7B/13B)\n",
        "\n",
        "End-to-end Colab to generate LLaVA-1.5 answers and evaluate with GPT-4o.\n",
        "\n",
        "Recommended: Runtime > Change runtime type > GPU (A100 preferred; T4 works).\n",
        "\n",
        "Notes:\n",
        "- Uses HuggingFace llava-hf (1.5) models with bf16/fp16 precision (no 4-bit).\n",
        "- Judge uses OpenAI GPT-4o; set your OPENAI_API_KEY when prompted.\n",
        "- If OpenAI SDK issues on Colab: we pin httpx<0.28.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "gpu_check"},
      "source": [
        "import torch, os\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "    try:\n",
        "        print('Capability:', torch.cuda.get_device_capability(0))\n",
        "    except Exception as e:\n",
        "        print('Capability: unknown', e)\n",
        "else:\n",
        "    print('Using CPU (slower)')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {"id": "deps"},
      "source": [
        "%pip -q install \"transformers>=4.41\" \"accelerate>=0.30\" pillow requests \"openai>=1.37.0\" \"httpx<0.28\" bitsandbytes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {"id": "mount_drive"},
      "source": [
        "# Optional: mount Google Drive if your dataset/images are in Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {"id": "clone_repo"},
      "source": [
        "# Clone the Socrates repo (adjust if you have a fork)\n",
        "REPO_URL = 'https://github.com/MohammedEsamaldin/Socrates.git'\n",
        "WORKDIR = '/content/Socrates'\n",
        "import os, sys\n",
        "if not os.path.exists(WORKDIR):\n",
        "    !git clone $REPO_URL $WORKDIR\n",
        "%cd $WORKDIR\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {"id": "params"},
      "source": [
        "# Set paths and parameters\n",
        "# Point DATASET to your MMHal-Bench JSON (list of 96 records).\n",
        "# If records have relative image paths, set IMAGE_ROOT accordingly.\n",
        "import os\n",
        "DATASET = '/content/path/to/mmhal.json'  # TODO: replace\n",
        "IMAGE_ROOT = '/content/path/to/images'   # TODO: replace or set to '' if absolute/URLs\n",
        "OUTPUT = '/content/llava15_7b_responses.json'\n",
        "EVAL_JSON = '/content/llava15_7b_eval.json'\n",
        "MODEL_ID = os.environ.get('SOC_LLAVA_MODEL', 'llava-hf/llava-1.5-7b-hf')  # or 'llava-hf/llava-1.5-13b-hf'\n",
        "MAX_NEW_TOKENS = 256\n",
        "TEMPERATURE = 0.2\n",
        "print('MODEL_ID =', MODEL_ID)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {"id": "generate"},
      "source": [
        "# Generate answers with LLaVA-HF (bf16/fp16 enforced, no 4-bit)\n",
        "!python -m socrates_system.mllm_evaluation.scripts.generate_mmhal_llava_hf \\\n",
        "  --dataset \"$DATASET\" \\\n",
        "  --image-root \"$IMAGE_ROOT\" \\\n",
        "  --output \"$OUTPUT\" \\\n",
        "  --model-id \"$MODEL_ID\" \\\n",
        "  --max-new-tokens $MAX_NEW_TOKENS \\\n",
        "  --temperature $TEMPERATURE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {"id": "set_key"},
      "source": [
        "# Set your OpenAI API key for judging (kept in env only within this session)\n",
        "import os, getpass\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass('Enter OPENAI_API_KEY: ')\n",
        "print('Key set:', 'OPENAI_API_KEY' in os.environ)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {"id": "judge"},
      "source": [
        "# Judge with GPT-4o (structured results saved to EVAL_JSON)\n",
        "!python -m socrates_system.mllm_evaluation.analysis.MMHal_Bench.eval_gpt4 \\\n",
        "  --response \"$OUTPUT\" \\\n",
        "  --evaluation \"$EVAL_JSON\" \\\n",
        "  --api-key \"$OPENAI_API_KEY\" \\\n",
        "  --gpt-model gpt-4o-2024-08-06\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {"id": "summary"},
      "source": [
        "# Quick summary of evaluation results\n",
        "import json\n",
        "from statistics import mean\n",
        "with open(EVAL_JSON, 'r', encoding='utf-8') as f:\n",
        "    eval_recs = json.load(f)\n",
        "ratings = [r.get('rating', 0) for r in eval_recs]\n",
        "hall = [r.get('hallucination', 1) for r in eval_recs]\n",
        "print('Num samples:', len(eval_recs))\n",
        "print('Average score:', round(mean(ratings), 2) if ratings else 0)\n",
        "print('Hallucination rate:', round(mean(hall), 2) if hall else 1.0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Troubleshooting\n",
        "- If VRAM is insufficient, switch to 7B or reduce max tokens.\n",
        "- For dataset schema, the generator expects keys: question, image (or image_path, etc.), image_content (optional).\n",
        "- The judge script expects fields: image_id, question_type, question_topic, image_content, question, gt_answer, model_answer.\n",
        "- If using URLs for images, set IMAGE_ROOT to '' and ensure records contain HTTP(S) URLs.\n"
      ]
    }
  ]
}
